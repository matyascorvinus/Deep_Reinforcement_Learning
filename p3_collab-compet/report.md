[//]: # (Image References)

[image1]: https://user-images.githubusercontent.com/10624937/42135623-e770e354-7d12-11e8-998d-29fc74429ca2.gif "Trained Agent"
[image2]: https://user-images.githubusercontent.com/10624937/42135622-e55fb586-7d12-11e8-8a54-3c31da15a90a.gif "Soccer"

Introduction

For this project, you will work with the [Tennis](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#tennis) environment.

![Trained Agent][image1]

In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.

The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation. After that, the Environment will condense 3 time frames of observation space together, make the number of the state of the environment for each agents is 24.

Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. 



The task is episodic, and in order to solve the environment, your agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents). Specifically,

- After each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores.
- This yields a single **score** for each episode.

The environment is considered solved, when the average (over 100 episodes) of those **scores** is at least +0.5.


The Algorithm:
In the final project of Udacity's Deep Reinforcement Learning, I use the Multi Agents Deep Deterministic Policy Gradient (MADDPG) algorithm. This algorithm shares almost the same architecture with the Deep Deterministic Policy Gradient (DDPG), as each agents has the local-target Actor network for deciding actions based on the states of the environment, and the local-target Critic network used for creating the Q_Values states - actions table used to guide and correct the Actor network into the right direction.

The local networks will directly involve with the training and the execution of the agents in the environment.
The target networks are useful as they add the delay between the time the agent has updated their weight and when the update has actually affected the agent --> provide a safety net for the agent to update the weight of the network. 


However, the way how the critic networks update and correct the actor network is much different in MADDPG than in DDPG. Let me explain why:

![imag](https://github.com/matyascorvinus/Deep_Reinforcement_Learning/blob/master/p3_collab-compet/The%20DDPG%20Algorithm.png)

Figure of DDPG Algorithm

In DDPG Algorithm, for each agents in the environment, the target critic network of that agent will create the Q_Values table based on two parameters:
* The future states, based on the perspective of that agent from the environment (which is seperated from the general states the whole environment generated). 
* The actions created from the same sates by the target Actor Network.

Then they use the Bellman equation to calculate the Q_target for the current states (the predictor Q_target).

Finally, to update critic by minimizing the loss, we calculate the loss function between the expected Q_values (generated from local critic network) and the Q_target for the current states (generated by the target critic), finally using policy gradient method to update the local critic network.  (The loss calculated by the mean squared errors function)

With the Actor network, the actor is updated using the sampled policy gradient: 
* First we calculate the loss function, by using the local critic network to generated the Q_values from the current state and the current action created from the same state by the local actor network, then we find the mean values from the Q_values.
* Then we use the policy gradient method to update the weight of the local actor network.

![imag](https://github.com/matyascorvinus/Deep_Reinforcement_Learning/blob/master/p3_collab-compet/Overview_MADDPG.png)

![imag](https://github.com/matyascorvinus/Deep_Reinforcement_Learning/blob/master/p3_collab-compet/MADDPG_Algorithm.png)

Figure of MADDPG Algorithm

IN MADDPG, for each agent networks, the local-target critic networks is centralized - we centralize the network by combining the whole states of the environment which represent all the perspective of all agents together. (So now both the states and the next states will represent the whole environment) (1)

![imag](https://github.com/matyascorvinus/Deep_Reinforcement_Learning/blob/master/p3_collab-compet/Update%20the%20Critic%20Network.png)

The Critic Networks:
* Critic Network used the actions and the states to generate Q_values table. So for the actions, we also concatenate the actions (both current actions created from all agents and next actions generated from the target actor network of the agent) values of all agents together as well. (2)

* With the combined might of the states (from (1)) and the actions (from (2)), each agent will use the target critic network to calculate the Q_target tables and therefore calculate the loss function to update the weight of the critic network with the gradient descent step method on the loss. (The loss calculated by the mean squared errors function).


![imag](https://github.com/matyascorvinus/Deep_Reinforcement_Learning/blob/master/p3_collab-compet/Update%20the%20Actor%20Network.png)
The Actor networks:
* In the decision making processes of the Actor network, I used the clip(-1,1) function to narrow the interval of the actions, as it is continuous actions decision making.
* Almost the same as the Critic one - we combined the current actions created from all agents and current action generated from the local actor network of the agent. (3)
* We calculate the loss: by the average of the Q_values table generated by local critic network and using the parameters from the combined current states (from (1)) and the combined actions (from (3)).
* Finally we use the gradient descent step method to update the weight of the actor network.

In both the MADDPG and the DDPG, I use many techniques such as Experience Replay : saving the states, actions, rewards, ... to a memory buffer, and then use Prioritize technique to shuffle the memory, so the agent will not lean into the rabbit hole when learning. To stabilize the update, I did not 100% update the weight of the agents, but using soft update where the target weights get a small fraction of the target weughts combined with a bigger portion from the local weights.

![imag](https://github.com/matyascorvinus/Deep_Reinforcement_Learning/blob/master/p2_continuous-control/Actor-Critic.png)

![imag](https://github.com/matyascorvinus/Deep_Reinforcement_Learning/blob/master/p2_continuous-control/Actor-Critic%20DDPG%20Architecture.png)

The Actor-Critic Networks: Basically it is the same as the DDPG Algorithm

![imag](https://github.com/matyascorvinus/Deep_Reinforcement_Learning/blob/master/p3_collab-compet/MADDPG%20Architecture.png)

The Architecture of the MADDPG Algorithm Implementation

![imag](https://github.com/matyascorvinus/Deep_Reinforcement_Learning/blob/master/p3_collab-compet/MADDPG%20Agent%20class.png)


![imag](https://github.com/matyascorvinus/Deep_Reinforcement_Learning/blob/master/p3_collab-compet/MADDPG%20Manager%20class.png)

MADDPG Manager

![imag](https://github.com/matyascorvinus/Deep_Reinforcement_Learning/blob/master/p3_collab-compet/MADDPG%20Results.png)

The MADDPG Results



Future plans: I will finish the Soccer exercise using the same MADDPG algorithm with Prioritized Experience Replay and Soft Update technique. Then I will do the Finance exercise using DDPG Algorithm and continue the learning there. Furthermore, I will explore other Unity ML-Agents environments to master my skills in implementing Reinforcement Learning Algorithm
